# To run the DSC on the GWDG cluster, connect to a GWDG login
# node, and run this:
#
#  dsc --host gwdg.yml linreg.dsc
# 
# Note that you have to load the modules first
# module load gcc (L0Learn was installed using gcc/10.2.0)
# module load intel-parallel-studio/cluster.2020.4 (R installed with MKL)
#
# R and Python could be loaded as modules in frontends and sbatch jobs
# module load R/4.0.4
# conda activate /cbscratch/sbanerj/software/python/envs/py39
#
# Non-interactive shell logins in GWDG do not load user ~/.bashrc
# Hence, loading conda and local modules are not being processed by jobs.
# Hence I used the following hack:
#     source /usr/users/sbanerj/.bashrc
#
# Each exclusive GWDG node has 16 cores and 128GB memory.
# Maximum of 1024 pipeline module calculationss are submitted 
# in each DSC job (with threading -c 16),
# which averages to 64 jobs per core.
# Around 10min per module would require 10h for a job
# Around 2min per module would require 2h for a job
#
# Error on GWDG:
# Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.
#	Try to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.
# export MKL_SERVICE_FORCE_INTEL=1
# export OMP_NUM_THREADS=1
#

DSC:
  gwdg:
    address: localhost
    queue_type: pbs
    status_check_interval: 60
    max_running_jobs: 100
    task_template: |
      #!/bin/bash
      #SBATCH -A cramer
      #SBATCH -p em
      #SBATCH --time=2-00:00:00
      #SBATCH --exclusive
      #SBATCH -N 1
      #SBATCH -o slurm-out/slurm-%j.out
      #SBATCH -e slurm-out/slurm-%j.err
      source /usr/users/sbanerj/.bashrc
      module load gcc/9.3.0
      module load intel-parallel-studio/cluster.2020.4
      module load R/4.0.4
      conda activate /cbscratch/sbanerj/software/python/envs/py39
      export MKL_SERVICE_FORCE_INTEL=1
      export OMP_NUM_THREADS=16
    submit_cmd: sbatch {job_file}
    submit_cmd_output: "Submitted batch job {job_id}"
    status_cmd: squeue --job {job_id}
    kill_cmd: scancel {job_id}

default:
  queue: gwdg
  instances_per_job: 1024
  instances_per_node: 16
  cpus_per_instance: 1
  time_per_instance: '10:00:00'
