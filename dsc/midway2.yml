# To run the DSC on the midway2 cluster, connect to a midway2 login
# node, and run this:
#
#  dsc --host midway2.yml linreg.dsc
# 
# Note that you have to load the modules first
# module load R
# module load gcc (L0Learn was installed using gcc/10.2.0)
# conda activate py38
#
#
# #####
# DSC queue options
# #####
# instances_per_job
#   Number of module instances for each job. Include many for short-running instances
# nodes_per_job
#   Multi-node processing via MPI. Not used here.
# cpus_per_instance
#   How many CPUs for each module instance, should be equal to OMP_NUM_THREADS
# instances_per_node
#   How many parallel module instances allowed on each node.
#   instances_per_node * cpus_per_instance should be equal to --cpus_per_task
#   Max 28 for mstephens / broadwl in RCC
#   I used 4 so that the jobs do not have to wait for exclusive nodes
# time_per_instance
#   Max computation time for each module instance
# mem_per_instance
#   Max memory for each module instance
#
# Error on midway2:
# Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.
#	Try to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.
# export MKL_SERVICE_FORCE_INTEL=1
# export OMP_NUM_THREADS=1
#

DSC:
  midway2:
    address: localhost
    queue_type: pbs
    status_check_interval: 60
    max_running_jobs: 100
    task_template: |
      #!/bin/bash
      #SBATCH --partition mstephens
      #SBATCH --account pi-mstephens
      #SBATCH --time=4-00:00:00
      #SBATCH --nodes=1
      #SBATCH --cpus-per-task=4
      #SBATCH --mem=36G
      #SBATCH -o slurm-out/slurm-%j.out
      #SBATCH -e slurm-out/slurm-%j.err
      source ~/.bashrc
      module load gcc/10.2.0
      module load mkl/2020.up1
      module load R/3.6.1
      conda activate py39
      export MKL_SERVICE_FORCE_INTEL=1
      export OMP_NUM_THREADS=1
    submit_cmd: sbatch {job_file}
    submit_cmd_output: "Submitted batch job {job_id}"
    status_cmd: squeue --job {job_id}
    kill_cmd: scancel {job_id}

default:
  queue: midway2
  instances_per_job: 1024
  instances_per_node: 4
  cpus_per_instance: 1
  time_per_instance: '4-00:00:00'
